{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 앙상블\n",
    "\n",
    "Ref. https://swalloow.github.io/bagging-boosting\n",
    "\n",
    "- ### 앙상블이란\n",
    "    - 약한 학습기 (weak trainer)를 조합하여 성능이 높은 분류기를 만드는 방법\n",
    "    - 약한 학습기의 종류에 따라 2개로 분류 할 수 있다. (Bagging, Boosting)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- ### Bagging\n",
    "\n",
    "    - 샘플을 여러 번 각 모델에 학습시켜 결과를 집계(Aggregating)하는 방식\n",
    "    - 약한 학습기 각각을 복원 랜덤 샘플링한 데이터로 학습시킴 <br/> -> 모델의 예측변수들을 집계 <br/>-> 그 결과로 모델을 생성\n",
    "    \n",
    "    \n",
    "- Bagging의 장점    \n",
    "    - 대부분 학습에서 나타나는 오류는 다음 두 가지다.\n",
    "        - 높은 bias로 인한 underfitting\n",
    "        - 높은 Variance로 인한 overfitting\n",
    "    \n",
    "    - Bagging은 overfitting을 방지하기 효과적이다.\n",
    "        - 각 샘플의 결과를 집계하므로 중간 값을 사용한다고 볼 수 있음\n",
    "        \n",
    "    - Bagging은 데이터를 일반화하여 분류하는 모델에 적합\n",
    "    \n",
    "    \n",
    "- Bagging의 집계 방식\n",
    "    - 집계 방식\n",
    "        - Categorical data면 voting으로 집계\n",
    "        - Continuous data면 Average로 집계\n",
    "    - RandomForest 모델이 대표적\n",
    "        - 단일 Decision Tree모델은 discrete(1차)한 모양이지만,\n",
    "        - 여러 트리 모델을 결합하여 높은 차원의 모델(n차)로 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Boosting\n",
    "\n",
    "    - Bagging과 다르게 어려운 문제를 맞추는 데 초점이 맞춰짐\n",
    "    - Bagging과 다르게 순차적으로 학습시킵니다.\n",
    "    - 오답에 높은 가중치를 부여하여 오답에 focusing함.\n",
    "    - 정확도가 높은 대신 outlier에 취약\n",
    "    - 대표적인 모델로 AdaBoost, XGBoost, GradientBoost가 있음\n",
    "    \n",
    "    \n",
    "- 방식\n",
    "    1. 지도 학습 데이터 <br/>\n",
    "    2. 약한 학습기의 가중치 초기화\n",
    "    3. 약한 학습기에서 판별 실시\n",
    "    4. 약한 학습기에서 잘못 판별한 지도학습 데이터\n",
    "    5. 난이도 높은 지도학습 데이터\n",
    "    6. 지도학습 데이터 가중치 변경\n",
    "    *.다른 약한 학습기에 위의 과정을 적용. 반복.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Stacking\n",
    "\n",
    "Meta Modeling 이라고 불리기도 하는 이 방법은 위의 2가지 방식과는 조금 다릅니다. “Two heads are better than one” 이라는 아이디어에서 출발합니다.\n",
    "\n",
    "Stacking은 서로 다른 모델들을 조합해서 최고의 성능을 내는 모델을 생성합니다. 여기에서 사용되는 모델은 SVM, RandomForest, KNN 등 다양한 알고리즘을 사용할 수 있습니다. 이러한 조합을 통해 서로의 장점은 취하고 약점을 보완할 수 있게 되는 것 입니다.\n",
    "\n",
    "Stacking은 이미 느끼셨겠지만 필요한 연산량이 어마어마합니다. 적용해보고 싶다면 아래의 StackNet을 사용하시는 걸 강력하게 추천합니다.\n",
    "\n",
    "https://github.com/kaz-Anova/StackNet\n",
    "\n",
    "문제에 따라 정확도를 요구하기도 하지만, 안정성을 요구하기도 합니다. 따라서, 주어진 문제에 적절한 모델을 선택하는 것이 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
