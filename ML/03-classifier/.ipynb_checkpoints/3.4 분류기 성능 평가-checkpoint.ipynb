{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 분류기 성능 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 분류기 성능을 측정하는 여러가지 방법이 존재\n",
    "- Accuracy, Precision, Recall, F1 score \n",
    "- Ref : https://sumniya.tistory.com/26\n",
    "\n",
    "\n",
    "- Confusion matrix\n",
    "    - 분류의 결과를 실재와 예측 결과를 비교.\n",
    "    - True Pos, Neg / False Pos, Neg로 나눔\n",
    "    - 익숙한 표인 느낌을 받는다면 Type 1, 2 error표와 유사\n",
    "    \n",
    "||예측||\n",
    "|------|---|---|\n",
    "||Positive|Negative|\n",
    "|실재|True Positive (TP)|False Negative (FN)|\n",
    "||False Positive (FP)|True Negative (TN)|\n",
    "\n",
    "\n",
    "- 정답률 (Accuracy)\n",
    "    - 전체 경우 중 True Pos, Neg의 비율\n",
    "    - 실제와 예측이 일치하는 경우만\n",
    "    - form -> (TP + TN) / (TP + FP + FN + TN)\n",
    "    - Domain 특성상 True인 경우가 많은 bias가 존재한다면 TN의 비율이 낮을 수 밖에 없음을 고려해야함\n",
    "\n",
    "\n",
    "- 적합률 (Precision)\n",
    "    - PPV(Positive preditive value)로도 불림\n",
    "    - 예측 Positive의 경우 중 True Positive의 비율\n",
    "    - form -> TP / (TP + FP)\n",
    "    - 데이터가 적을수록 적합률이 높다.\n",
    "    - Example : 날씨 예측 모델이 맑다로 예측했는데 실제 날씨가 맑았는지\n",
    "\n",
    "\n",
    "- 재현율 (Recall)\n",
    "    - 통계학에서는 sensitivity, 다른 분야에서는 hit rate\n",
    "    - 실제 Positive의 경우 중 True Positive의 비율\n",
    "    - form -> TP / TP + FN\n",
    "    - 데이터가 많을수록 재현률이 높다.\n",
    "    - Example : 실제 맑은 날 중 모델이 맑다고 예측한 비율\n",
    "\n",
    "\n",
    "- F1 score\n",
    "    - Precision과 Recall에 지우치지 않게 평균을 낸 성능평가 지표\n",
    "    - 조화평균을 사용하면 Precision과 Recall 중 작은 쪽에 치우친 평균이 도출된다. 산술평균을 이용하는 것보다 큰 쪽에서 끼치는 bias가 줄어든다.\n",
    "    - F1 = 2 / {(1/precision) + (1/Recall)}\n",
    "\n",
    "\n",
    "- 그 외 지표들\n",
    "    - Fall-out (False Positive Rate) = FP / (TN + FP)\n",
    "    - ROC (receiver operating characteristic) curve\n",
    "        - Recall-Fallout의 변화를 비교\n",
    "        - 실제 Pos인 경우 중 TP와 FP 비율을 비교\n",
    "        - Curve의 형태, Recall(TP)가 높게 나타날수록 높은 성능\n",
    "    - AUC (Area under curve)\n",
    "        - ROC curve는 그래프이기 때문에, 명확한 수치로써 비교하기 어려움.\n",
    "        - AUC는 그래프 아래의 면접의 최대 값을 성능 지표로. Recall이 높을수록 높은 성능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2 MNIST 성능 살펴보기\n",
    "simple_classifier 아래 부분 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * 번외. 그래서 뭘 써야할까\n",
    "\n",
    "- 적합률과 재현율은 trade-off 관계\n",
    "    - 오류 검출을 막으려고 예측 정확도가 높은 것만 골라내면 recall이 낮아진다.\n",
    "    - 정답을 놓칠 확률을 낮추어, 정확도가 낮은 예측도 가능성을 남겨두면 그만큼 전체 예측률이 낮아집니다. 빗나갈 확률이 높아지죠. precision이 낮아짐.\n",
    "    - 두 가지 수치를 보고 레이블의 종합적인 분류 성능을 판단할 수 있어야한다.\n",
    "\n",
    "\n",
    "- 그래서.\n",
    "    - precision : 예측이 맞을 확률을 높이고 싶다면.\n",
    "    - recall : 실제 상황 속에서 맞는 예측을 많이 남기고 싶으면.\n",
    "    - F-value : 종합적인 분류 성능을 높이고 싶다면\n",
    "\n",
    "\n",
    "- Example. (이해가 안된다면)\n",
    "    - 당신은 구글의 검색를 총괄하고 있습니다. 다양한 유저가 검색을 하고 검색 결과를 받아보고 있죠. 각 수치가 상징하는 바는 이렇습니다.\n",
    "    - precision : 유저가 찾는 검색어에 높은 일치률을 가진 결과를 보여준다.\n",
    "    - recall : 유저가 찾는 검색어와 조금이라도 연관된 결과를 최대한 많이 보여준다.\n",
    "    - F value : 둘의 종합적인 분류 성능을 보여줄 때 사용한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
